{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Presence - Retrieval Augmented Generation Model Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Small Intro\n",
    "### TODO: Intro into RAGs\n",
    "### TODO: Intro into Business Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Optional Part for Advanced students, to do some data wrangling\n",
    "### AKA: Correct Document loading, cleansing, transforming intermediate steps -> \n",
    "### Maybe make it replicate a small BRONZE-SILVER-GOLD set up -> \n",
    "#   bronze is raw documents\n",
    "#   silver is prefiltered documents based on type\n",
    "#   gold is documents filtered into the correct data type.\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents[filename] = file.read()\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "directory_path = [...] # W\n",
    "documents = load_documents_from_directory(directory_path)\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Function to create embeddings\n",
    "def create_embeddings(documents, model_name='sentence-transformers/all-MiniLM-L6-v2', embedding_type='sentence'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = {}\n",
    "    \n",
    "    for doc_name, doc_content in documents.items():\n",
    "        if embedding_type == 'character':\n",
    "            embeddings[doc_name] = model.encode([c for c in doc_content]) # Modify this\n",
    "        elif embedding_type == 'word':\n",
    "            embeddings[doc_name] = model.encode(doc_content.split()) # Modify this\n",
    "        elif embedding_type == 'sentence':\n",
    "            embeddings[doc_name] = model.encode(doc_content.split('.')) # Modify this\n",
    "        elif embedding_type == 'document':\n",
    "            embeddings[doc_name] = model.encode([doc_content]) # Modify this\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding type: {embedding_type}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "embeddings = create_embeddings(documents, embedding_type='document')\n",
    "print(f\"Created embeddings for {len(embeddings)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your_pinecone_api_key', environment='your_pinecone_environment')\n",
    "\n",
    "# Create Pinecone index\n",
    "index_name = 'rag-workshop'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # TODO: Modify this such that students might have to add their own dimensions and the index name should change.\n",
    "    pinecone.create_index(index_name, dimension=384)  # Adjust dimension according to your model\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Function to add embeddings to Pinecone\n",
    "def add_embeddings_to_pinecone(embeddings):\n",
    "    for doc_name, embedding in embeddings.items():\n",
    "        index.upsert([(doc_name, embedding.tolist())])\n",
    "    \n",
    "# Example usage\n",
    "add_embeddings_to_pinecone(embeddings)\n",
    "print(\"Embeddings added to Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize a language model pipeline\n",
    "llm_pipeline = pipeline('text-generation', model='gpt-3.5-turbo')  # Replace with appropriate model\n",
    "\n",
    "# Function to query the vector database and generate a response\n",
    "def query_rag_system(query, top_k=5):\n",
    "    query_embedding = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').encode([query])\n",
    "    search_results = index.query(query_embedding.tolist(), top_k=top_k)\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_docs = [result['id'] for result in search_results['matches']]\n",
    "    \n",
    "    # Generate response using LLM\n",
    "    context = \" \".join([documents[doc_id] for doc_id in retrieved_docs])\n",
    "    response = llm_pipeline(f\"Context: {context}\\nQuestion: {query}\\nAnswer: \")\n",
    "    \n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the content of document X?\"\n",
    "response = query_rag_system(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have a working RAG (Retrieval-Augmented Generation) Model, we will want our customers/users to be able to retrieve documents freely using natural language.\n",
    "\n",
    "However, it is often the case that these RAG Models perform better when given prompts that provide additional context and guidance in the role they are trying to fulfill.\n",
    "\n",
    "# Understanding Prompt Engineering\n",
    "Prompt engineering is the practice of designing and refining the input prompts given to a language model to achieve the desired output. Essentially, it involves crafting questions or statements in a way that leverages the strengths of the language model, guiding it to produce more accurate, relevant, and contextually appropriate responses. This is crucial for improving the performance of RAG systems, where the quality of retrieved documents and generated responses can directly impact user satisfaction and correctness.\n",
    "\n",
    "We will now be looking at the benefits of prompt engineering and how we can modify the following query to enhance our RAG Model performance.\n",
    "\n",
    "## Benefits of Prompt Engineering\n",
    "\n",
    "Consinder the query: \"Tell me about climate change.\"\n",
    "\n",
    "it is quite an open-ended and ambigious prompt, leaving the opportunity for the RAG model to hallucinate and generat potentially misleading, useless or incorrect responses. We can _mitigate_ this by employing certain technniques in our prompts.\n",
    "\n",
    "### Contextual Understanding: \n",
    "\n",
    "Prompts that provide specific context can help the language model better understand the user's intent. This reduces ambiguity and ensures that the model retrieves and generates content that is closely aligned with the user's needs.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Tell me about climate change and its impact on coastal cities.\"\n",
    "\n",
    "### Enhanced Relevance: \n",
    "\n",
    "By framing prompts to include relevant details, users can improve the relevance of the documents retrieved from the vector database. This ensures that the information presented is more pertinent to the query.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Tell me about the effects of climate change on agriculture in North America.\"\n",
    "\n",
    "### Role Specification: \n",
    "\n",
    "Defining the role of the model within the prompt (e.g., \"As an expert in history, summarize the events of World War II\") can help the model generate responses that are more authoritative and tailored to the specified role.\n",
    "\n",
    "__Modified Query__:\n",
    "\"As an environmental scientist, explain the causes and effects of climate change.\"\n",
    "\n",
    "###  Guidance and Structure: \n",
    "\n",
    "Structured prompts (e.g., \"Given the following context, provide a summary: [context]\") guide the model on how to approach the response, which can lead to more coherent and well-organized outputs.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Given the following context, provide a summary of the main points about climate change: [context]\"\n",
    "\n",
    "### Bias Mitigation: \n",
    "\n",
    "Thoughtfully crafted prompts can help mitigate model biases by steering the model towards neutral and objective language, particularly in sensitive or controversial topics.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Provide an objective overview of climate change, including its causes, effects, and potential solutions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Prompt Engineering to our RAG Model.\n",
    "\n",
    "It would be too much to ask from our users/customers to apply all these techniques themselves when they are querying the system for information. Therefore, many applications that employ RAG models do some additional preprocessing to user prompts to leverage the benefits of Prompt Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
