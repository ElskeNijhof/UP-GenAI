{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Presence - Retrieval Augmented Generation Model Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Intro into Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Overview\n",
    "Retrieval-Augmented Generation (RAG) is an advanced framework that combines retrieval-based and generation-based approaches to enhance the performance of natural language processing (NLP) tasks. It leverages the strengths of both methods to provide more accurate, relevant, and contextually appropriate responses. This hybrid approach is particularly powerful in scenarios where vast amounts of information need to be efficiently accessed and summarized, such as in question answering systems, customer support, and knowledge management.\n",
    "\n",
    "## Components of RAG\n",
    "1. **Document Retrieval:**\n",
    "   - The first step involves retrieving relevant documents or passages from a large corpus of text. This is typically done using vector databases and similarity search techniques. The aim is to narrow down the vast information to a few relevant pieces that can be further processed.\n",
    "\n",
    "2. **Embedding Models:**\n",
    "   - Embedding models transform text data into numerical vectors that capture semantic meanings. Various embedding techniques can be used, such as character-level, word-level, sentence-level, and document-level embeddings. The choice of embedding type depends on the specific use case and the desired level of granularity.\n",
    "\n",
    "3. **Vector Database (e.g., Pinecone):**\n",
    "   - A vector database stores and indexes these embeddings, enabling efficient similarity searches. Pinecone, for instance, is a scalable vector database that supports high-dimensional vector search, making it ideal for real-time retrieval tasks in RAG systems.\n",
    "\n",
    "4. **Language Model (LLM) Prompting:**\n",
    "   - Once the relevant documents are retrieved, a language model (such as GPT-3.5 or similar) generates a response based on the retrieved context and the user's query. This step involves prompt engineering to guide the model in producing high-quality outputs.\n",
    "\n",
    "## How RAG Works\n",
    "1. **Query Processing:**\n",
    "   - The user inputs a query. This query is embedded using an embedding model to create a vector representation.\n",
    "   \n",
    "2. **Retrieval Step:**\n",
    "   - The query vector is used to search the vector database, retrieving the most similar documents or passages. This narrows down the information to the most relevant pieces.\n",
    "\n",
    "3. **Generation Step:**\n",
    "   - The retrieved documents are fed into a language model along with the original query. The language model uses this context to generate a coherent and relevant response.\n",
    "\n",
    "4. **Response Delivery:**\n",
    "   - The generated response is presented to the user, providing a comprehensive answer or summary based on the combined knowledge of the retrieved documents and the language model's generative capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Idea: Create Multiple Datasets  -> Different topics (i.e. Tech documentation, Finance, Law, Ethics and Compliance, Health, etc). Each group gets to pick a dataset to present a business case (How would you sell it to a client?)\n",
    "### Idea: More Complex - Evaluate different similarity metrics, and parameters. -> Maybe different metrics https://www.pinecone.io/learn/vector-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your_pinecone_api_key', environment='your_pinecone_environment')\n",
    "\n",
    "# Model Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Global Variables that should not be modified once run.\n",
    "group_name = \"YOUR_GROUP_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the corpus/data we want to create a vector database from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_documents_from_directory(directory_path, file_extensions : str | list = '.txt'): # TODO: Add logic for file extensions\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents[filename] = file.read()\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "directory_path = './sources/data/{topic}'.format(topic=\"\")\n",
    "file_extensions = [\".txt\", \".pdf\"]\n",
    "documents = load_documents_from_directory(directory_path)\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings for the documents we want to find information in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Function to create embeddings\n",
    "model_name  = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "def create_embeddings(documents: list = [], model : any = None, embedding_type: str = 'sentence'):\n",
    "\n",
    "    if (len(documents) == 0):\n",
    "        raise ValueError('You have provided an empty Document List')\n",
    "    \n",
    "    if (model is None):\n",
    "        raise TypeError('You have provided an invalid model or it has not been loaded correctly')\n",
    "    \n",
    "    embeddings = {}\n",
    "\n",
    "    for doc_name, doc_content in documents.items():\n",
    "        if embedding_type == 'character':\n",
    "            embeddings[doc_name] = model.encode([c for c in doc_content]) # Modify this\n",
    "        elif embedding_type == 'word':\n",
    "            embeddings[doc_name] = model.encode(doc_content.split()) # Modify this\n",
    "        elif embedding_type == 'sentence':\n",
    "            embeddings[doc_name] = model.encode(doc_content.split('.')) # Modify this\n",
    "        elif embedding_type == 'document':\n",
    "            embeddings[doc_name] = model.encode([doc_content]) # Modify this\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding type: {embedding_type}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "document_embeddings = create_embeddings(documents, embedding_type='document')\n",
    "sentence_embeddings = create_embeddings(documents, embedding_type='document')\n",
    "word_embeddings = create_embeddings(documents, embedding_type='word')\n",
    "character_embeddings = create_embeddings(documents, embedding_type='character')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Vector Database with the vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_size = 256 \n",
    "# Create Pinecone index\n",
    "def create_pinecode_index(group_name, dimension):\n",
    "    index_name = 'rag-workshop' + group_name\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        pinecone.create_index(index_name, dimension=dimension)  # Adjust dimension according to your model\n",
    "    index = pinecone.Index(index_name)\n",
    "    return index\n",
    "\n",
    "# Function to add embeddings to Pinecone\n",
    "def add_embeddings_to_pinecone(index, embeddings):\n",
    "    for doc_name, embedding in embeddings.items():\n",
    "        index.upsert([(doc_name, embedding.tolist())])\n",
    "    \n",
    "\n",
    "pinecone_index = create_pinecode_index(group_name, chunk_size)\n",
    "add_embeddings_to_pinecone(embeddings)\n",
    "print(\"Embeddings added to Pinecone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the documents which are the most semantically similar to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize a language model pipeline\n",
    "llm_pipeline = pipeline('text-generation', model='gpt-3.5-turbo')  # Replace with appropriate model\n",
    "\n",
    "# Function to query the vector database and generate a response\n",
    "def retrieve_similar_documents(query, top_k=5):\n",
    "    query_embedding = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').encode([query])\n",
    "    search_results = index.query(query_embedding.tolist(), top_k=top_k)\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_docs = [result['id'] for result in search_results['matches']]\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a response using a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_system(documents: list, query: str):\n",
    "\n",
    "    # Generate response using LLM\n",
    "    context = \" \".join([documents[doc_id] for doc_id in documents])\n",
    "    response = llm_pipeline(f\"Context: {context}\\nQuestion: {query}\\nAnswer: \")\n",
    "    \n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have a working RAG (Retrieval-Augmented Generation) Model, we will want our customers/users to be able to retrieve documents freely using natural language.\n",
    "\n",
    "However, it is often the case that these RAG Models perform better when given prompts that provide additional context and guidance in the role they are trying to fulfill.\n",
    "\n",
    "# Understanding Prompt Engineering\n",
    "Prompt engineering is the practice of designing and refining the input prompts given to a language model to achieve the desired output. Essentially, it involves crafting questions or statements in a way that leverages the strengths of the language model, guiding it to produce more accurate, relevant, and contextually appropriate responses. This is crucial for improving the performance of RAG systems, where the quality of retrieved documents and generated responses can directly impact user satisfaction and correctness.\n",
    "\n",
    "We will now be looking at the benefits of prompt engineering and how we can modify the following query to enhance our RAG Model performance.\n",
    "\n",
    "## Benefits of Prompt Engineering\n",
    "\n",
    "Consinder the query: \"Tell me about climate change.\"\n",
    "\n",
    "it is quite an open-ended and ambigious prompt, leaving the opportunity for the RAG model to hallucinate and generat potentially misleading, useless or incorrect responses. We can _mitigate_ this by employing certain technniques in our prompts.\n",
    "\n",
    "### Contextual Understanding: \n",
    "\n",
    "Prompts that provide specific context can help the language model better understand the user's intent. This reduces ambiguity and ensures that the model retrieves and generates content that is closely aligned with the user's needs.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Tell me about climate change and its impact on coastal cities.\"\n",
    "\n",
    "### Enhanced Relevance: \n",
    "\n",
    "By framing prompts to include relevant details, users can improve the relevance of the documents retrieved from the vector database. This ensures that the information presented is more pertinent to the query.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Tell me about the effects of climate change on agriculture in North America.\"\n",
    "\n",
    "### Role Specification: \n",
    "\n",
    "Defining the role of the model within the prompt (e.g., \"As an expert in history, summarize the events of World War II\") can help the model generate responses that are more authoritative and tailored to the specified role.\n",
    "\n",
    "__Modified Query__:\n",
    "\"As an environmental scientist, explain the causes and effects of climate change.\"\n",
    "\n",
    "###  Guidance and Structure: \n",
    "\n",
    "Structured prompts (e.g., \"Given the following context, provide a summary: [context]\") guide the model on how to approach the response, which can lead to more coherent and well-organized outputs.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Given the following context, provide a summary of the main points about climate change: [context]\"\n",
    "\n",
    "### Bias Mitigation: \n",
    "\n",
    "Thoughtfully crafted prompts can help mitigate model biases by steering the model towards neutral and objective language, particularly in sensitive or controversial topics.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Provide an objective overview of climate change, including its causes, effects, and potential solutions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Prompt Engineering to our RAG Model.\n",
    "\n",
    "It would be too much to ask from our users/customers to apply all these techniques themselves when they are querying the system for information. Therefore, many applications that employ RAG models do some additional preprocessing to user prompts to leverage the benefits of Prompt Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
